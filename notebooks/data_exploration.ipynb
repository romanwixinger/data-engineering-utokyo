{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b2ba97",
   "metadata": {},
   "source": [
    "# Data exploration for joining all data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611ee33",
   "metadata": {},
   "source": [
    "In this notebook we develop the functions for retrieving the tables, preparing them for the join, and then combining them based on the timestamp. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f28370",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0454d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import collections\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6deb4d2",
   "metadata": {},
   "source": [
    "## Sources\n",
    "In the following section we construct the methods to read the different csv files and convert them to flat tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1588e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c49f147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Helper():\n",
    "    \n",
    "    @classmethod\n",
    "    def timestamp_to_datetimes(cls, df: pd.DataFrame): \n",
    "        \"\"\" Takes a pandas dataframe with a timestamp column (int) and also adds date datetime, datetime_ms, datetime_μs. \n",
    "        \"\"\"\n",
    "        # Conversion functions\n",
    "        conversion_to_datetime_μs = lambda x: datetime.datetime.fromtimestamp(x/1000000000).strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        conversion_to_datetime_ms = lambda x: datetime.datetime.fromtimestamp(x/1000000000).strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "        conversion_to_datetime = lambda x: datetime.datetime.fromtimestamp(x/1000000000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Apply conversions\n",
    "        df[\"datetime_μs\"] = df[\"timestamp\"].apply(conversion_to_datetime_μs)\n",
    "        df[\"datetime_ms\"] = df[\"timestamp\"].apply(conversion_to_datetime_ms)\n",
    "        df[\"datetime\"] = df[\"timestamp\"].apply(conversion_to_datetime)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6932467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder(object): \n",
    "    \"\"\" Base class for mapping a csv file to a pandas dataframe in real-time when it changes. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath: str): \n",
    "        # Settings\n",
    "        self.filepath = filepath\n",
    "        \n",
    "        # Tracking\n",
    "        self.read_data_lines = 0\n",
    "        self.last_updated = 0\n",
    "        self.data_last_updated = 0\n",
    "        self.metadata_last_updated = 0\n",
    "        \n",
    "        # Dataframes \n",
    "        self._table_df = None     # Data x Metadata\n",
    "        self._data_df = None      # Data\n",
    "        self._metadata_df = None  # Metadata\n",
    "\n",
    "    def get_table(self) -> pd.DataFrame: \n",
    "        self._update()\n",
    "        self.last_updated = self._get_mod_time()\n",
    "        return self._table_df\n",
    "    \n",
    "    def get_data(self) -> pd.DataFrame: \n",
    "        self._update_data()\n",
    "        self.data_last_updated = self._get_mod_time()\n",
    "        return self._data_df\n",
    "    \n",
    "    def get_metadata(self) -> pd.DataFrame: \n",
    "        self._update_metadata()\n",
    "        self.metadata_last_updated = self._get_mod_time()\n",
    "        self.metadata_columns = list(self._metadata_df.columns)\n",
    "        return self._metadata_df\n",
    "    \n",
    "    def table_is_up_to_date(self) -> bool:\n",
    "        return self.last_updated == self._get_mod_time()\n",
    "    \n",
    "    def data_is_up_to_date(self) -> bool: \n",
    "        return self.data_last_updated == self._get_mod_time()\n",
    "    \n",
    "    def metadata_is_up_to_date(self) -> bool: \n",
    "        return self.metadata_last_updated == self._get_mod_time()\n",
    "    \n",
    "    def _update(self): \n",
    "        if self.table_is_up_to_date(): \n",
    "            return \n",
    "        data_df = self.get_data()\n",
    "        metadata_df = self.get_metadata()\n",
    "        self._table_df = self._data_df.merge(self._metadata_df, how='cross') if self._metadata_df is not None else self._data_df\n",
    "        self._harmonize_time()\n",
    "        self._table_df = self._table_df.sort_values(by=\"timestamp\")\n",
    "        self.last_updated = self._get_mod_time()\n",
    "        \n",
    "    def _get_mod_time(self): \n",
    "        return time.ctime(os.path.getmtime(self.filepath))\n",
    "        \n",
    "    def _update_data(self):\n",
    "        \"\"\" Updates self._data_df and self.data_last_updated incrementally. Makes use of _load_new_data(). \n",
    "        \"\"\"\n",
    "        \n",
    "        # Case first loading \n",
    "        if self.read_data_lines == 0: \n",
    "            self._data_df = self._load_new_data()\n",
    "            return \n",
    "        \n",
    "        # Case reloading\n",
    "        new_data_df = self._load_new_data()\n",
    "        if len(new_data_df.index) == 0: \n",
    "            return\n",
    "\n",
    "        # Concatenate old and new\n",
    "        self._data_df = pd.concat(\n",
    "            [self._data_df, new_data_df],\n",
    "            axis=0,\n",
    "            join=\"outer\",\n",
    "            ignore_index=True,\n",
    "            copy=True\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _load_new_data(self) -> pd.DataFrame: \n",
    "        \"\"\" Returns the rows which have not been loaded so far. \n",
    "        \"\"\"\n",
    "    \n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _update_metadata(self): \n",
    "        \"\"\" Updates self.metadata_df and self.metadata_last_updated. We do a full reload. \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _harmonize_time(self): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a24aa5",
   "metadata": {},
   "source": [
    "### SSD2 Data\n",
    "Data from the WE7000 DAQ for the SSD2. The PMT current from the MOT will be obtained like this in our next experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a08c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecorderSSD(Recorder): \n",
    "    \"\"\" Class for data engineering of the SSD2 data. \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str):\n",
    "        super(RecorderSSD, self).__init__(filepath)\n",
    "        self.nr_meta_data_rows = 37\n",
    "\n",
    "    def _load_new_data(self) -> pd.DataFrame: \n",
    "        new_data_df = pd.read_csv(filepath_or_buffer=self.filepath, \n",
    "                                 skiprows=self.nr_meta_data_rows + self.read_data_lines, \n",
    "                                 header=0, \n",
    "                                 names=[\"TraceName\", \"Time_x\", \"PulseHeight\"])        \n",
    "        self.read_data_lines += len(new_data_df.index)\n",
    "        return new_data_df\n",
    "        \n",
    "    def _update_metadata(self): \n",
    "        with open(self.filepath, newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            metadata = list(reader)[:(self.nr_meta_data_rows + 1)]\n",
    "            metadata =  metadata[:3] +  metadata[4:]\n",
    "            columns = [line[0] for line in metadata]\n",
    "            row = [line[1] for line in metadata]\n",
    "            self._metadata_df = pd.DataFrame(data=[row], columns=columns)\n",
    "                \n",
    "    def _harmonize_time(self): \n",
    "        \"\"\" Convert the relative time and start time to the real time. \"\"\"\n",
    "        \n",
    "        def harmonize_table(df: pd.DataFrame) -> pd.DataFrame: \n",
    "            # Start time\n",
    "            helper_df = pd.DataFrame()\n",
    "            helper_df[\"start_datetime_str\"] = df[\"//StartDate\"].apply(lambda s: s.replace(\"/\", \"-\")) + \" \" + df[\"//StartTime\"]\n",
    "            helper_df[\"start_datetime\"] = pd.to_datetime(helper_df[\"start_datetime_str\"]) \n",
    "\n",
    "            # Conversion parameter: Time_x * rel_time_to_ns = rel. time in ns\n",
    "            time_resolution = df['//TimeResolution'][0]\n",
    "            rel_time_to_ns = {\n",
    "                '1.000000e-009': 1e-0,\n",
    "                '1.000000e-006': 1e+3,\n",
    "                '1.000000e-003': 1e+6\n",
    "            }[time_resolution]\n",
    "\n",
    "            # Real time\n",
    "            helper_df[\"relative_time_ns\"] = df[\"Time_x\"] * rel_time_to_ns\n",
    "            helper_df[\"start_ns\"] = helper_df.start_datetime.values.astype(np.int64)\n",
    "            helper_df[\"timestamp\"] = helper_df[\"start_ns\"] + helper_df[\"relative_time_ns\"]\n",
    "\n",
    "            # Add datetimes\n",
    "            df[\"timestamp\"] = helper_df[\"timestamp\"]\n",
    "            Helper.timestamp_to_datetimes(df)\n",
    "            return df\n",
    "        \n",
    "        self._table_df = harmonize_table(self._table_df)\n",
    "        self._data_df = self._table_df[self._data_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b33c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorderSDD = RecorderSSD(location+\"-20220314-100806-Slot1-In2.csv\")\n",
    "ssd_df = recorderSDD.get_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorderSDD.get_table()\n",
    "recorderSDD.read_data_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d1d455",
   "metadata": {},
   "source": [
    "### SSD Histogram representation\n",
    "A very useful representation of the SSD data is when we group by time (primary key) and then count the number of pulses with a certain hight. The columns then correspond to voltage ranges in 12-bit (0, 1, 2, 3, ..., 4094, 4095) encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a54251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDRepresentation(): \n",
    "    \n",
    "    @classmethod\n",
    "    def get_hist_rep(cls, ssd_df: pd.DataFrame) -> pd.DataFrame: \n",
    "        nr_of_channels = ssd_df[\"//Gain\"][0]\n",
    "        \n",
    "        # Groupby timestamp and PulseHeight (channel)\n",
    "        grouped = ssd_df.groupby(by=[\"datetime\", \"PulseHeight\"])\n",
    "        df = grouped.agg({\"TraceName\": 'count'}).rename({\"TraceName\": \"PulseCount\"}, axis=1)\n",
    "        df = df.unstack(level='PulseHeight').fillna(0, downcast=\"infer\")\n",
    "        \n",
    "        # Fix column names\n",
    "        df.columns = df.columns.to_flat_index()\n",
    "        df.columns = [str(col[1]) for col in df.columns.values]\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def get_time_aggregated_hist_rep(cls, ssd_df: pd.DataFrame) -> pd.DataFrame: \n",
    "        df = cls.get_hist_rep(ssd_df) \n",
    "        df_sum = df.sum()\n",
    "        df = pd.DataFrame(data=[df_sum.values], columns=df_sum.index.values)\n",
    "        return df\n",
    "    \n",
    "    def get_channel_aggregated_hist_rep(cls, ssd_df: pd.DataFrame) -> pd.DataFrame: \n",
    "        \"\"\" TODO: Implement aggregation over all channels. \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d1fae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>...</th>\n",
       "      <th>3543</th>\n",
       "      <th>3556</th>\n",
       "      <th>3661</th>\n",
       "      <th>3676</th>\n",
       "      <th>3719</th>\n",
       "      <th>3842</th>\n",
       "      <th>3929</th>\n",
       "      <th>3977</th>\n",
       "      <th>3984</th>\n",
       "      <th>4000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-03-14 19:07:54</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-14 19:07:56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-14 19:07:57</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-14 19:07:58</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-14 19:07:59</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-14 21:06:45</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-14 21:06:47</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-14 21:06:48</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-14 21:06:49</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-14 21:06:50</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6294 rows × 1617 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     500  501  502  503  504  505  506  507  508  509  ...  \\\n",
       "datetime                                                               ...   \n",
       "2022-03-14 19:07:54    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "2022-03-14 19:07:56    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "2022-03-14 19:07:57    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "2022-03-14 19:07:58    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "2022-03-14 19:07:59    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "...                  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "2022-03-14 21:06:45    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "2022-03-14 21:06:47    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "2022-03-14 21:06:48    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "2022-03-14 21:06:49    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "2022-03-14 21:06:50    0    0    0    0    0    0    0    0    0    0  ...   \n",
       "\n",
       "                     3543  3556  3661  3676  3719  3842  3929  3977  3984  \\\n",
       "datetime                                                                    \n",
       "2022-03-14 19:07:54     0     0     0     0     0     0     0     0     0   \n",
       "2022-03-14 19:07:56     0     0     0     0     0     0     0     0     0   \n",
       "2022-03-14 19:07:57     0     0     0     0     0     0     0     0     0   \n",
       "2022-03-14 19:07:58     0     0     0     0     0     0     0     0     0   \n",
       "2022-03-14 19:07:59     0     0     0     0     0     0     0     0     0   \n",
       "...                   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "2022-03-14 21:06:45     0     0     0     0     0     0     0     0     0   \n",
       "2022-03-14 21:06:47     0     0     0     0     0     0     0     0     0   \n",
       "2022-03-14 21:06:48     0     0     0     0     0     0     0     0     0   \n",
       "2022-03-14 21:06:49     0     0     0     0     0     0     0     0     0   \n",
       "2022-03-14 21:06:50     0     0     0     0     0     0     0     0     0   \n",
       "\n",
       "                     4000  \n",
       "datetime                   \n",
       "2022-03-14 19:07:54     0  \n",
       "2022-03-14 19:07:56     0  \n",
       "2022-03-14 19:07:57     0  \n",
       "2022-03-14 19:07:58     0  \n",
       "2022-03-14 19:07:59     0  \n",
       "...                   ...  \n",
       "2022-03-14 21:06:45     0  \n",
       "2022-03-14 21:06:47     0  \n",
       "2022-03-14 21:06:48     0  \n",
       "2022-03-14 21:06:49     0  \n",
       "2022-03-14 21:06:50     0  \n",
       "\n",
       "[6294 rows x 1617 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssd_hist_df = SSDRepresentation.get_hist_rep(ssd_df)\n",
    "ssd_hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7417399d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'SSDRepresentation' has no attribute 'get_fully_aggregated_hist_rep'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3508/4098344366.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mssd_full_agg_hist_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSSDRepresentation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_fully_aggregated_hist_rep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mssd_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mssd_full_agg_hist_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'SSDRepresentation' has no attribute 'get_fully_aggregated_hist_rep'"
     ]
    }
   ],
   "source": [
    "ssd_full_agg_hist_df = SSDRepresentation.get_fully_aggregated_hist_rep(ssd_df)\n",
    "ssd_full_agg_hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f860b",
   "metadata": {},
   "source": [
    "### PMT Data\n",
    "In the table, we have the following columns. \n",
    "- No. : Frame number of CMOS camera\n",
    "- Time: When the frame is obtained\n",
    "- PMT Current: Current from photomultiplier at the time the frame obtained\n",
    "- ROI Sum: The sum of signal values in ROI\n",
    "- Coil (1:ON 0:ODD): The current of the coil at that time\n",
    "\n",
    "Comment: \n",
    "- Will be depraced and replaced by the SSD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04802939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecorderPMT(): \n",
    "    \n",
    "    def __init__(self, filepath: str):\n",
    "        super(RecorderPMT, self).__init__(filepath)\n",
    "    \n",
    "    def _update_data(self, filepath: str=location+\"all_data.csv\"): \n",
    "        df = pd.read_csv(filepath_or_buffer=filepath)\n",
    "        df.rename({\"Unnamed: 5\": \"a\"}, axis=\"columns\", inplace=True)\n",
    "        df = df.drop([\"a\"], axis=1)\n",
    "        cls._harmonize_time(df)\n",
    "        df = df.sort_values(by=\"timestamp\")\n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def _harmonize_time(self): \n",
    "        \n",
    "        self._table_df[\"datetime_μs\"] = self._table_df[\"Time\"].apply(lambda s: s+\"000\")\n",
    "        self._table_df[\"datetime_ms\"] = self._table_df[\"Time\"]\n",
    "        df[\"datetime\"] = df[\"Time\"].apply(lambda s: s[:-4])\n",
    "        df[\"timestamp\"] = df[\"Time\"].apply(pd.Timestamp).values.astype(np.int64)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorderPMT = RecorderPMT(\"all_data.csv\")\n",
    "pmt_df = recorderPMT.get_table()\n",
    "pmt_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89cb708",
   "metadata": {},
   "source": [
    "### Coil Log\n",
    "The current of the MOT coil is controlled by a relay switch. This text file is the log of the relay switch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c364f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coil(): \n",
    "    \n",
    "    @classmethod\n",
    "    def get_table(cls, filepath: str=location+\"coil_log.txt\"): \n",
    "        df = pd.read_csv(filepath_or_buffer=filepath, delimiter=\"\t\")\n",
    "        cls._harmonize_time(df)\n",
    "        df = df.sort_values(by=\"timestamp\")\n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def _harmonize_time(cls, df: pd.DataFrame): \n",
    "        df[\"datetime_μs\"] = df[\"Time\"].apply(lambda s: s+\"000\")\n",
    "        df[\"datetime_ms\"] = df[\"Time\"]\n",
    "        df[\"datetime\"] = df[\"Time\"].apply(lambda s: s[:-4])\n",
    "        df[\"timestamp\"] = df[\"Time\"].apply(pd.Timestamp).values.astype(np.int64)\n",
    "        return \n",
    "\n",
    "coil_table = Coil.get_table()\n",
    "coil_table "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ba9c1",
   "metadata": {},
   "source": [
    "### Heater Log\n",
    "Log of the IR heater output percentage for target heating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450cc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Heater(): \n",
    "    \"\"\" Class for data engineering of the heater data. \"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_table(cls, filepath: str=location+\"HeaterLog_20220314_100740_00001.csv\", with_metadata: bool=False): \n",
    "        \"\"\" Load full table, consisting of data and metadata in a flat format. \"\"\"\n",
    "        data_df = cls._get_data(filepath)\n",
    "        metadata_df = cls._get_metadata(filepath)\n",
    "        df = data_df.merge(metadata_df, how='cross')\n",
    "        cls._harmonize_time(df)\n",
    "        if with_metadata: \n",
    "            return df\n",
    "        return df[[col for col in df.columns if col not in metadata_df.columns]]    \n",
    "    \n",
    "    @classmethod\n",
    "    def _get_data(cls, filepath: str=location+\"HeaterLog_20220314_100740_00001.csv\") -> pd.DataFrame: \n",
    "        df = pd.read_csv(filepath_or_buffer=filepath, \n",
    "                                 skiprows=6, \n",
    "                                 header=0, \n",
    "                                 names=[\"Date\", \"Time\", \"Unknown\", \"TargetPercentage\", \"MeasuredPercentage\"],\n",
    "                                 encoding=\"cp932\")\n",
    "        cls._harmonize_time(df)\n",
    "        df = df.sort_values(by=\"timestamp\")\n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def _get_metadata(cls, filepath: str=location+\"HeaterLog_20220314_100740_00001.csv\"): \n",
    "        with open(filepath, newline='', encoding=\"cp932\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            metadata_list = list(reader)[:6]\n",
    "            columns = [m[0] for m in metadata_list]\n",
    "            row = [metadata_list[i][1] for i in range(2)] +  [f\"{metadata_list[i][3]},{metadata_list[i][4]}\" for i in range(2, 6)]\n",
    "            df = pd.DataFrame(data=[row], columns=columns)\n",
    "            return df\n",
    "    \n",
    "    @classmethod\n",
    "    def _harmonize_time(cls, df: pd.DataFrame) -> pd.DataFrame: \n",
    "        df[\"datetime\"] = df[\"Date\"].apply(lambda s: s.replace(\"/\", \"-\")) + \" \" + df[\"Time\"]\n",
    "        df[\"datetime_μs\"] = df[\"datetime\"].apply(lambda s: s+\".000000\")\n",
    "        df[\"datetime_ms\"] = df[\"datetime\"].apply(lambda s: s+\".000\")\n",
    "        df[\"timestamp\"] = df[\"datetime_ms\"].apply(pd.Timestamp).values.astype(np.int64)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04ade9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "heater_df = Heater.get_table(with_metadata=False)\n",
    "heater_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908f3b1",
   "metadata": {},
   "source": [
    "### Ion Beam Control\n",
    "Log of the Fr ion source. First day of experiment, right after the end of primary beam check, just starting the Fr ion extraction. The column \"FC\" is the current from either one of the faraday cups, or the sum of both.\n",
    "The columns \"Center\" and \"Surrounding\" are the voltages applied to the mechanical relay switches that connects the faraday cups to the picoammeter. For example, if \"Center\" = 24 and \"Surrounding\" = 0, the value at \"FC\" is the current observed on FC Center in nA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fe746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class IonBeamControl(): \n",
    "    \n",
    "    @classmethod\n",
    "    def get_table(cls, filepath: str=location+\"IonBeamControl1.5_DESKTOP-8ICG2TJ_20220314_114132.csv\"): \n",
    "        df = pd.read_csv(filepath_or_buffer=filepath)\n",
    "        cls._harmonize_time(df)\n",
    "        df = df.sort_values(by=\"timestamp\")\n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def _harmonize_time(cls, df: pd.DataFrame) -> pd.DataFrame: \n",
    "        df[\"datetime\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "        df[\"datetime_μs\"] = df[\"Timestamp\"].apply(lambda s: s+\".000000\")\n",
    "        df[\"datetime_ms\"] = df[\"Timestamp\"].apply(lambda s: s+\".000\")\n",
    "        df[\"timestamp\"] = df[\"datetime\"].values.astype(np.int64)\n",
    "        \n",
    "ion_beam_control_df = IonBeamControl.get_table()\n",
    "ion_beam_control_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227daa4",
   "metadata": {},
   "source": [
    "### Gauge Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44427be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gauge(): \n",
    "    \n",
    "    @classmethod\n",
    "    def get_table(cls, filepath: str=location+\"TPG256GaugeMonitor_Single_DESKTOP-BEF5FI4_20220312_203214.csv\"): \n",
    "        df = pd.read_csv(filepath_or_buffer=filepath)\n",
    "        cls._harmonize_time(df)\n",
    "        df = df.sort_values(by=\"timestamp\")\n",
    "        return df\n",
    "    \n",
    "    @classmethod\n",
    "    def _harmonize_time(cls, df: pd.DataFrame): \n",
    "        df[\"datetime\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "        df[\"datetime_μs\"] = df[\"Timestamp\"].apply(lambda s: s+\".000000\")\n",
    "        df[\"datetime_ms\"] = df[\"Timestamp\"].apply(lambda s: s+\".000\")\n",
    "        df[\"timestamp\"] = df[\"datetime\"].values.astype(np.int64)\n",
    "        return df\n",
    "\n",
    "gauge_df = Gauge.get_table()\n",
    "gauge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0112d5c6",
   "metadata": {},
   "source": [
    "### Laser data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Laser(): \n",
    "    \n",
    "    @classmethod\n",
    "    def get_table(cls, filepath: str=location+\"15.03.2022, 21.30, 384.22817013 THz.lta\", with_metadata: bool=False): \n",
    "        \"\"\" Load full table, consisting of data and metadata in a flat format. \"\"\"\n",
    "        data_df = cls._get_data(filepath)\n",
    "        metadata_df = cls._get_metadata(filepath)\n",
    "        if not with_metadata: \n",
    "            metadata_df = metadata_df[[\"StartTime\"]]\n",
    "        df = data_df.merge(metadata_df, how='cross')\n",
    "        cls._harmonize_time(df)\n",
    "        df = df.sort_values(by=\"timestamp\")\n",
    "        if with_metadata: \n",
    "            return df\n",
    "        else:\n",
    "            return df[[col for col in df.columns if col not in metadata_df.columns]]     \n",
    "    \n",
    "    @classmethod\n",
    "    def _get_data(cls, filepath: str=location+\"15.03.2022, 21.30, 384.22817013 THz.lta\") -> pd.DataFrame:\n",
    "        \"\"\" Loads the data and makes sure that each row has all laser measurements by aggregating rows. \"\"\"\n",
    "        original_df = pd.read_csv(filepath_or_buffer=filepath, \n",
    "                                  skiprows=119,\n",
    "                                  delimiter=\"\t\")\n",
    "        \n",
    "        df = cls._aggregate_laser_rows(original_df)\n",
    "        return df\n",
    "        \n",
    "    @classmethod\n",
    "    def _aggregate_laser_rows(cls, original_df: pd.DataFrame): \n",
    "        \"\"\" Originally, one measurement of the six laser wavelengths is distributed over six rows. We aggregate \n",
    "            these rows into one row. The only tradeoff is that we have to approximate the time with the time\n",
    "            of the last measurement. \n",
    "        \"\"\"\n",
    "        \n",
    "        n = len(original_df.index)\n",
    "        \n",
    "        time_column = 'Time  [ms]'\n",
    "        laser_columns = original_df.columns[1:]\n",
    "        n_laser_columns = len(laser_columns)\n",
    "        \n",
    "        laser_column_lookup = {col: i for i, col in enumerate(laser_columns)}\n",
    "        data_lookup = collections.defaultdict(list)\n",
    "        row_lookup = {}\n",
    "        row_list = []\n",
    "        \n",
    "        for index, row in original_df.iterrows():\n",
    "            column_index = pd.Series.first_valid_index(row[1:])\n",
    "            row_lookup[column_index] = row[column_index]\n",
    "            # Count if all 6 lasers have been measured\n",
    "            if len(row_lookup) == 6:     \n",
    "                item = [row[time_column]] + [row_lookup[col] for col in laser_columns]\n",
    "                row_list.append(item)\n",
    "                row_lookup = {}\n",
    "\n",
    "        df = pd.DataFrame(data=row_list, columns=original_df.columns)\n",
    "        return df\n",
    "        \n",
    "    @classmethod\n",
    "    def _get_metadata(cls, filepath: str=location+\"15.03.2022, 21.30, 384.22817013 THz.lta\"): \n",
    "        with open(filepath, newline='', encoding=\"cp932\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\t\")\n",
    "            metadata_list = list(reader)[:119]\n",
    "            \n",
    "            # Title\n",
    "            title_column = [\"Title\"]\n",
    "            title_row = [metadata_list[0][0]]\n",
    "            \n",
    "            # General info\n",
    "            gi_columns = [m[0] for m in metadata_list[1:7]]\n",
    "            gi_rows = [cls._combine(m[1:]) for m in metadata_list[1:7]]\n",
    "            \n",
    "            # General settings\n",
    "            gs_columns = [m[0] for m in metadata_list[9:20]]\n",
    "            gs_rows = [cls._combine(m[1:]) for m in metadata_list[9:20]]\n",
    "\n",
    "            # Frames 1-6\n",
    "            frame_columns = (\n",
    "                [m[0] for m in metadata_list[22:36]]\n",
    "                + [m[0] for m in metadata_list[38:52]]\n",
    "                + [m[0] for m in metadata_list[54:68]]\n",
    "                + [m[0] for m in metadata_list[70:84]]\n",
    "                + [m[0] for m in metadata_list[86:100]]\n",
    "                + [m[0] for m in metadata_list[102:116]]\n",
    "            )\n",
    "            \n",
    "            frame_rows = (\n",
    "                [cls._combine(m[1:]) for m in metadata_list[22:36]]\n",
    "                + [cls._combine(m[1:]) for m in metadata_list[38:52]]\n",
    "                + [cls._combine(m[1:]) for m in metadata_list[54:68]]\n",
    "                + [cls._combine(m[1:]) for m in metadata_list[70:84]]\n",
    "                + [cls._combine(m[1:]) for m in metadata_list[86:100]]\n",
    "                + [cls._combine(m[1:]) for m in metadata_list[102:116]]\n",
    "            )\n",
    "            \n",
    "            columns = title_column + gi_columns + gs_columns + frame_columns\n",
    "            row = title_row + gi_rows + gs_rows + frame_rows\n",
    "            df = pd.DataFrame(data=[row], columns=columns)\n",
    "            return df                                    \n",
    "    \n",
    "    @classmethod\n",
    "    def _combine(cls, entries: list): \n",
    "        \"\"\" If entries has length 1, then it returns the entry. \n",
    "            Otherwise, it converts the list to a comma separated string. \n",
    "        \"\"\"\n",
    "        if len(entries) == 0: \n",
    "            return None\n",
    "        \n",
    "        if len(entries) == 1: \n",
    "            return entries[0]\n",
    "        \n",
    "        return \",\".join(entries)\n",
    "    \n",
    "    @classmethod\n",
    "    def _harmonize_time(cls, df: pd.DataFrame):\n",
    "\n",
    "        # Convert 15.03.2022, 08:46:39.387 to 15-03-2022 08:46:39.387\n",
    "        helper_df = pd.DataFrame()\n",
    "        helper_df[\"StartTime\"] = df[\"StartTime\"].apply(lambda s: s.replace(\",\", \"\").replace(\".\", \"-\", 2))\n",
    "        \n",
    "        # Calculate absolute time based on relative time\n",
    "        helper_df[\"start_datetime\"] = pd.to_datetime(helper_df[\"StartTime\"])\n",
    "        helper_df[\"start_timestamp\"] = helper_df[\"start_datetime\"].values.astype(np.int64)\n",
    "        helper_df[\"timestamp\"] = helper_df[\"start_timestamp\"] + df[\"Time  [ms]\"] * 1e3\n",
    "        \n",
    "        # Add datetimes\n",
    "        df[\"timestamp\"] = helper_df[\"timestamp\"]\n",
    "        Helper.timestamp_to_datetimes(df)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c07877",
   "metadata": {},
   "outputs": [],
   "source": [
    "laser_df = Laser.get_table(with_metadata=False)\n",
    "laser_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5901b56d",
   "metadata": {},
   "source": [
    "### Image data\n",
    "We have a folder which contains the picture data from the CMOS camera. A part of the pixels (region of interest, ROI) selected in advance is extracted. Each picture data is a csv file and records signals from each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image(): \n",
    "    \n",
    "    def get_array(filepath: str=location+\"cmos_000039.csv\"): \n",
    "        with open(filepath) as file_name:\n",
    "            return np.loadtxt(file_name, delimiter=\",\")\n",
    "        \n",
    "    def get_metadata(filepath: str=location+\"cmos_000039.csv\") -> pd.DataFrame: \n",
    "        \n",
    "        columns = [\"size\", \"ctime\"] # Size and creation time\n",
    "        row = [os.path.getsize(filepath), os.path.getctime(filepath)]\n",
    "        return pd.DataFrame(data=[row], columns=columns)\n",
    "        \n",
    "image_array = Image.get_array()\n",
    "image_table = Image.get_metadata()\n",
    "image_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c349e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf288465",
   "metadata": {},
   "source": [
    "## Joins\n",
    "For a refresher on joins, checkout this article: https://pandas.pydata.org/docs/user_guide/merging.html. Now our goal is to join the tables into a main table. We join on the timestamp and define the following rules: \n",
    "- We join on timestamp in seconds with an outer join.\n",
    "- As the values do not exactly match, we first sort the dataframes, such that we then can join on nearly-matching values with some threshold (https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.merge_asof.html). \n",
    "\n",
    "Problem: \n",
    "- As the sampling method varies a lot, we would lose a lot of data with this method. \n",
    "\n",
    "Solution A: \n",
    "- Treat some tables as parameters and settings, and some tables as measurements. Then we first sample the parameters and settings on level second, and then use a finer sampling for the measurements. \n",
    "- For this, we first find all unique times in seconds. Then we generate the rows for each unique second, taking the value if it exists or nan if not. \n",
    "- Last, we do an outer join on the time in seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a86ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main(): \n",
    "\n",
    "    def limit_tables_to_timespan(dfs, start, stop): \n",
    "        \"\"\" Takes a list of dfs, only keeps the rows between start and stop time and returns the modified list. \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def build_table(dfs: list, prefixes: list): \n",
    "        \"\"\" Concatenates the dataframes to a single dataframe, ignoring the indices. The names are used as prefixes of the\n",
    "            columns, such that we can have similar column names but still know from which df it came. \n",
    "        \"\"\"   \n",
    "          \n",
    "        # Add prefixes\n",
    "        dfs = [df.rename(columns={col: prefix+\"_\"+col for col in df.columns}) for df, prefix in zip(dfs, prefixes)]\n",
    "        \n",
    "        # Outer join \n",
    "        main_df = pd.concat(dfs, axis=1, join=\"outer\")\n",
    "        \n",
    "        return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d122624",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table = Main.build_table(\n",
    "    dfs=[laser_df, gauge_df, ion_beam_control_df, heater_df, coil_table, pmt_df, ssd_df, ssd_hist_df],\n",
    "    prefixes=[\"laser\", \"gauge\", \"ion_beam_control\", \"heater\", \"coil\", \"pmt\", \"ssd\", \"ssd_hist\"]\n",
    ")\n",
    "main_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5673530",
   "metadata": {},
   "source": [
    "### Analysis \n",
    "In the following we want to test some simple analysis as proof of concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10897b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
